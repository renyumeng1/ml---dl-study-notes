# 6. 从全连接层到卷积

## 6.1 神经网络架构的特性

 1. *平移不变性（translation invariance）*:不管检测对象出现在图像的哪个位置，神经网络的前面基层应该对相同的图像区域具有相似的反应。（当待识别的对象不管出现在哪，识别器都不会有改变，与对象的位置无关。）
 2. *局部性（locality）*：神经网络前面几层应该只探索输入图像中的局部区域，而不过度在意图像中较远区域的关系。（只需要看到局部的信息。）

## 6.1.1 多层感知机的限制

$$
\begin{aligned}
\mathbf{H}_{i,j}&=[\mathbf{U}]_{i,j}+\sum_k\sum_l[\mathbf{W}]_{i,j,k,l}[\mathbf{X}]_{k,l}\\&=[\mathbf{U}]_{i,j}+\sum_a\sum_b[\mathbf{V}]_{i,j,a,b}[\mathbf{X}]_{i+a,j+b}.
\end{aligned}
$$

## 6.1.2 对卷积层的个人理解

卷积层省略了距离目标像素过远的信息，并且认为以该像素为中心的周围的像素的权重是一致的。所以卷积神经网络相比与多层感知机，可以减少参数的数量。多层感知机可能需要数十亿个参数来表示网络中的一层，而现在卷积神经网络通常只需要几百个参数，而且不需要改变输入或隐藏表示的维数。

## 6.1.3 总结

+ 卷积层将输入和核矩阵进行交叉相关，加上偏移后得到输出。
+ 核矩阵和偏移是可学习的参数。
+ 核矩阵的大小是超参数。
