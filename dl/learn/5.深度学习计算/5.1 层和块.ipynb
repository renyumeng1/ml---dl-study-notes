{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn,Tensor\n",
    "from d2l import torch as d2l\n",
    "from torch.nn import functional as f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0123, -0.0985,  0.0404, -0.0259, -0.0529, -0.2467,  0.1501, -0.0133,\n",
       "          0.2082,  0.0641],\n",
       "        [-0.0827, -0.2169,  0.1037, -0.0453, -0.0106, -0.1553,  0.0952, -0.0892,\n",
       "          0.1040, -0.0121]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "X = torch.rand(2,20)\n",
    "net(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self) -> None:\n",
    "    super().__init__()\n",
    "    self.hidden = nn.Linear(20,256)\n",
    "    self.out = nn.Linear(256,10)\n",
    "  # 定义模型的前向传播\n",
    "  def forward(self,X):\n",
    "    # 使用function中的relu\n",
    "    return self.out(f.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2782,  0.3424, -0.1423,  0.0821,  0.0352, -0.1648,  0.1165, -0.0476,\n",
       "          0.0192,  0.0324],\n",
       "        [-0.1214,  0.2150, -0.0512, -0.0778,  0.1232, -0.2033,  0.0953,  0.1267,\n",
       "         -0.0337, -0.0377]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "# 这里应该是使用了类重载\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "  def __init__(self,*args) -> None:\n",
    "    super().__init__()\n",
    "    for idx,module in enumerate(args):\n",
    "      self._modules[str(idx)] = module\n",
    "  def forward(self,X):\n",
    "    for block in self._modules.values():\n",
    "      X = block(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.3664e-02, -1.2101e-01,  1.3052e-01, -3.2957e-02,  9.6022e-02,\n",
       "          2.8757e-01,  6.5758e-02, -2.9576e-02, -6.9751e-02, -1.8845e-01],\n",
       "        [-1.7885e-02, -1.1425e-01,  1.4706e-01, -1.7229e-01, -7.3928e-02,\n",
       "          3.2578e-01, -9.9234e-05, -9.0535e-02, -2.9273e-02, -3.0240e-02]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(\n",
    "  nn.Linear(20,256),\n",
    "  nn.ReLU(),\n",
    "  nn.Linear(256,10)\n",
    ")\n",
    "    \n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "  def __init__(self) -> None:\n",
    "    super().__init__()\n",
    "    self.rand_weight = torch.rand((20,20),requires_grad=True)\n",
    "    self.linear = nn.Linear(20,20)\n",
    "  def forward(self,X:Tensor):\n",
    "    X = self.linear(X)\n",
    "    X = f.relu((X@self.rand_weight)+1)\n",
    "    X = self.linear(X)\n",
    "    while X.abs().sum() > 1:\n",
    "      X/=2\n",
    "    return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0638, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0290, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "  def __init__(self) -> None:\n",
    "    super().__init__()\n",
    "    self.net = nn.Sequential(\n",
    "      nn.Linear(20,64),nn.ReLU(),\n",
    "      nn.Linear(64,32)\n",
    "    )\n",
    "    self.linear = nn.Linear(32,16)\n",
    "  \n",
    "  def forward(self,X):\n",
    "    return self.linear(self.net(X))\n",
    "  \n",
    "chimera = nn.Sequential(\n",
    "  NestMLP(),\n",
    "  nn.Linear(16,20),\n",
    "  FixedHiddenMLP()\n",
    ")\n",
    "chimera(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
