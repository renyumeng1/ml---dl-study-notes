---
title: softmax回归笔记及部分公式推导
zhihu-tags: softmax回归, 深度学习, 机器学习
注意: 所有的冒号是半角冒号，冒号后面有一个半角空格
zhihu-url: https://zhuanlan.zhihu.com/p/694790714
---
#  $softmax$回归

参考资料:[d2l 3.4节](https://zh.d2l.ai/chapter_linear-networks/softmax-regression.html)

## $softmax$运算

为了将网络输出的值映射为概率，需要再对$o_n$做一次映射，映射公式如下：
$$
\begin{align}
\hat{y}_i = \frac{exp(o_j)}{\sum_k{exp(o_k)}}
\end{align}
$$
我们把这个映射函数称之为$softmax$函数，即：
$$
\hat{y}=softmax(\mathbf{o})
$$


### 为什么选择使用$e^{o_j}$作为映射的函数？

- 概率具有非负性，使用指数函数可以确保输出的值都是大于0的。
- 概率具有归一性，输出值之和应该等于一，所以需要做归一化。

## 小批量样本的矢量化

$softmax$回归的矢量计算表达式为：
$$
\begin{align}
O &=  XW +b \\
\hat{Y}&=softmax(O)
\end{align}
$$

## 对数似然

$$
-\log P(\mathbf{Y}\mid\mathbf{X})=\sum_{i=1}^{n}-\log P(\mathbf{y}^{(i)}\mid\mathbf{x}^{(i)})
$$

我们可以记$l(\mathbf{y},{\hat{\mathbf{y}}})=-\log P(\mathbf{y}\mid\mathbf{x})$，就有了书上式(3.4.7)

### 书式(3.4.7)$\Rightarrow$书式(3.4.8)

我们不妨假设真实值中表示的各个事件相互独立，用$p_i$表示$\hat{y}$中各个类别发生的概率，用$n_i$表示各事件发生的次数,用 $N$表示事件发生的总次数，则有：
$$
\begin{align}
P_{N个事件}(\mathbf{y}|\mathbf{x})&=p_1^{n_1}\cdot p_2^{n_2}\dots p_q^{n_q}\\
&=\prod_{j=1}^{q}{p_j^{n_j}}\\
&=\Big(\prod_{j=1}^{q}{p_j^{\frac{n_j}{N}}}\Big)^N \\
\end{align}
$$
而式中的$\frac{n_j}{N}$正是真实值$y$所对应的每个事件发生的概率，我们将其记为$y_j$，而在任意足够大的样本数下出现一个这样事件的概率为：
$$
\begin{align}
P(\mathbf{y}|\mathbf{x})&=\sqrt[N]{P_{N个事件}(\mathbf{y}|\mathbf{x})}\\
&=\prod_{j=1}^{q}{p_j^{y_j}}\\
&=\prod_{j=1}^{q}{\hat{y}_j^{y_j}}
\end{align}
$$
将其带入似然函数中可以得到：
$$
\begin{align}
l(\mathbf{y},\hat{\mathbf{y}}) &= -\log{\prod_{j=1}^{q}{\hat{y}_j^{y_j}}} \\
&=-\sum_{j=1}^{q}y_{j}\log\hat{y}_j
\end{align}
$$

### 为什么损失不使用平方损失而是交叉熵？

平方损失对于分类问题来说太严格了，我们并不需要预测的概率完全等于标签的概率，我们只需要知道属于其中一个类别的概率大于属于其他类别的概率就行了。而交叉熵只关心对正确类别的预测概率，因为只要其值足够大，就可以确保分类结果正确。

### 这里的交叉熵的含义是什么？
实际上这里的交叉熵就是选择出了$\hat{y}$预测出$y$所对应列别的概率。